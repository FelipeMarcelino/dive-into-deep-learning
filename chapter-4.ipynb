{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, the author introduces a few data manipulations.  These skills are necessary for any person entering in data science/machine learning.  Using PyTorch, and not MXnet, I had developed the same code/results showed in the book.  \n",
    "\n",
    "Linear algebra is the base of machine learning.  It gives us a robust set of techniques for working with tabular data.  Matrix operation is the core of machine learning,  principally using algorithms like backpropagation to optimization ours models parameters to fit our data as best possible, determining which way to optimize parameters of the models requires a little bit of calculus.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, data manipulation has two core tasks:\n",
    "- Acquire data\n",
    "- Process data\n",
    "Using the tensors of Pytorch, this kind of tool is necessary to store our data.  The tensors provide a few key advantages. First, it provides asynchronous computations using GPU and CPU.  Secondly, tensors can provide support for automatic differentiation. It is necessary for backpropagation and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter is focusing on getting you up and running with the basic functionality.  The next two chapters will be more concentrate on the math behind element-wise, normal distributions, and other essential operations.  In section 17.2, we have more in-depth mathematical content to be explored. <br><br>\n",
    "First, we need to import the Torch module.  In Torch we have tensors. Tensors are numerical arrays. It is like NDArrays of MXNet and can be stored in CPU or GPU. Tensors with two axes correspond to matrices, and arrays with more than two axes don't have any unique names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable x contains a tensor one-dimensional with length 12.  Another way to get the shape of a tensor is by using propriety .shape! If we have a two-dimensional array, the shape will be a tensor with two numbers! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function reshape allow us to change the shape of the tensors.  You can transform the one-dimensional tensor y into a matrix with shape (3,4).  If you don't want to make all calculations of dimensions, you can omit one dimension using the number -1 and writer the other dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "print(x.reshape((3,4)))\n",
    "print(x.reshape((-1,4)))\n",
    "print(x.reshape((3,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pytorch*** allows us to initialize tensors in multiple ways.  For example, you can initialize tensors with one's, zero's, or grab catch from memory.  The last method is more performative, but not too useful, because of the big numbers. Take a look of numbers [4.5710e-41, -2.4891e-37,  3.0899e-41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([0.0000e+00, 0.0000e+00, 7.0065e-45, 0.0000e+00, 8.9683e-44])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones(5))\n",
    "print(torch.zeros(5))\n",
    "print(torch.empty(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can randomly sample numbers from known distributions like gaussian or exponential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1.4635]), tensor([-0.7932]), tensor([-0.9193]), tensor([1.2948]), tensor([0.3475])]\n",
      "[tensor([0.0188]), tensor([0.8526]), tensor([0.6789]), tensor([1.2094]), tensor([0.0985])]\n"
     ]
    }
   ],
   "source": [
    "normal = torch.distributions.normal.Normal(torch.tensor([0.0]),torch.tensor([1.0]))\n",
    "exponential = torch.distributions.exponential.Exponential(torch.tensor([1.0]))\n",
    "sample_n = [normal.sample() for _ in range(5)]\n",
    "sample_e = [exponential.sample() for _ in range(5)]\n",
    "print(sample_n)\n",
    "print(sample_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet above, we created five samples of normal distributions with mean = 0 and std = 1($\\mathcal{N}(0,1)$) and five samples with exponential with rate = 1($\\lambda = 1$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([0, 1, 2, 3])\n",
      "y =  tensor([2, 2, 2, 2])\n",
      "x + y =  tensor([2, 3, 4, 5])\n",
      "x - y =  tensor([-2, -1,  0,  1])\n",
      "x * y =  tensor([0, 2, 4, 6])\n",
      "x / y =  tensor([0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "y = torch.ones_like(x) * 2\n",
    "print(\"x = \",x)\n",
    "print(\"y = \",y)\n",
    "print(\"x + y = \", x + y)\n",
    "print(\"x - y = \", x - y)\n",
    "print(\"x * y = \", x * y)\n",
    "print(\"x / y = \", x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations in Pytorch are element-wise. But, if you pay attention to div operation, it doesn't look right correct? It is because x and y are Long type tensors.  You have to transform these tensors in Float type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([0., 1., 2., 3.], dtype=torch.float64)\n",
      "y =  tensor([2., 2., 2., 2.], dtype=torch.float64)\n",
      "x + y =  tensor([2., 3., 4., 5.], dtype=torch.float64)\n",
      "x - y =  tensor([-2., -1.,  0.,  1.], dtype=torch.float64)\n",
      "x * y =  tensor([0., 2., 4., 6.], dtype=torch.float64)\n",
      "x / y =  tensor([0.0000, 0.5000, 1.0000, 1.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "y = torch.ones_like(x) * 2\n",
    "\n",
    "# Transformation\n",
    "x = x.double()\n",
    "y = y.double()\n",
    "\n",
    "print(\"x = \",x)\n",
    "print(\"y = \",y)\n",
    "print(\"x + y = \", x + y)\n",
    "print(\"x - y = \", x - y)\n",
    "print(\"x * y = \", x * y)\n",
    "print(\"x / y = \", x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many operations can be applied element-wise, such as exponentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0000,  2.7183,  7.3891, 20.0855], dtype=torch.float64)\n",
      "tensor([ 1.0000,  2.7183,  7.3891, 20.0855], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x = x.double()\n",
    "print(x.exp())\n",
    "print(torch.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***torch.mm*** allow us to made matrix operations. In the next code snippet, we create two matrices and transpose the second to make a dot multiplication between x and y. x has the shape (3,4), and y transpose has the shape(4,3), then creating a matrix with shape (3,3): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[2, 1, 4],\n",
      "        [1, 2, 3],\n",
      "        [4, 3, 2],\n",
      "        [3, 4, 1]])\n",
      "tensor([[ 18,  20,  10],\n",
      "        [ 58,  60,  50],\n",
      "        [ 98, 100,  90]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).reshape((3,4))\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(x)\n",
    "print(y.t())\n",
    "print(torch.mm(x,y.t()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another operations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [ 2,  1,  4,  3],\n",
      "        [ 1,  2,  3,  4],\n",
      "        [ 4,  3,  2,  1]])\n",
      "tensor([[ 0,  1,  2,  3,  2,  1,  4,  3],\n",
      "        [ 4,  5,  6,  7,  1,  2,  3,  4],\n",
      "        [ 8,  9, 10, 11,  4,  3,  2,  1]])\n",
      "tensor([[0, 1, 0, 1],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], dtype=torch.uint8)\n",
      "tensor(66)\n",
      "22.494443758403985\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat((x,y),0)) # Concatenation along axes 0\n",
    "print(torch.cat((x,y),1)) # Concatenation along axes 0\n",
    "print(x == y) # Binary statement: if x(i,j) == y(i,j) than 1, else 0\n",
    "print(x.sum())\n",
    "print(x.double().norm().item()) # Only for floating-point types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function .item() transform the tensor into python scalar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Broadcast Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]]) torch.Size([3, 1])\n",
      "tensor([[0, 1]]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "print(a,a.shape)\n",
    "print(b,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The broadcast mechanism is similar to Numpy. First, replicate the elements in rows and columns, so the two tensors have the same shape, and then apply the operations by elements.  Above Pytorch replicate column of tensor a and row b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 Indexing and slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Python array, elements in tensor can be accessed by its index.  One example, x[0:3] select the first element to last - 1 element, in that case, items [0,1,2] will be chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1,4)[1:3] # Matrix: Selects second and third row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1,4)[1:3,0:2] # Matrix: Selects second and third row and first and second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5, -1,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [-2, -2, -2, -2],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "x_diff = x.reshape(-1,4)\n",
    "x_diff[1,2] = -1\n",
    "print(x_diff)\n",
    "x_diff[1,:] = -2\n",
    "print(x_diff) #Assign multiple times in the second row and all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write elements of a matrix. Like above! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 Saving memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving memory is useful when we have restricted memory.  The last operations made in this notebooks, we always allocate new memory to host results.  In the example below,  y = x + y, the matrix pointed to y will be different after you get the result. Python id() function gives us the exact address of the referenced object in memory.  First, evaluates y + x, allocate new memory for the result and then subsequently redirects y to point at this new location in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(y)\n",
    "y = y.reshape(12) + x\n",
    "id(y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using inplace operations, we can have the same space of memory to store our results and avoid memory leak and unnecessarily allocation of memory.  Using zeros_likes, we clone the shape of a matrix and allocate zeros values into this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(z): 140120981356120\n",
      "id(z): 140120981356120\n"
     ]
    }
   ],
   "source": [
    "z = torch.zeros_like(y)\n",
    "print('id(z):', id(z))\n",
    "z[:] = x + y\n",
    "print('id(z):', id(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make even better use of memory, because of x + y here still allocate a temporary buffer to store x + y, we can directly invoke torch operations, avoiding temporary buffers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(z)\n",
    "torch.add(x, y, out=z)\n",
    "id(z) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to make in-place operations are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(x): 140120981561848\n",
      "tensor([0, 2, 4, 6]) id(x): 140120981561848\n",
      "id(x): 140121834222216\n",
      "tensor([0, 2, 4, 6]) id(x): 140121834222216\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "y = torch.arange(4)\n",
    "print(\"id(x):\", id(x))\n",
    "x += y\n",
    "print(x,\"id(x):\",id(x))\n",
    "x = torch.arange(4)\n",
    "y = torch.arange(4)\n",
    "print(\"id(x):\",id(x))\n",
    "x.add_(y)\n",
    "print(x,\"id(x):\",id(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!! Careful !!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-place operations for autograd is ***dangerous!*** <br>\n",
    "https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.6 Mututal Transformation of NDArray and NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last subsection is only a minor and easy example of converting a numpy array to tensor and vice-versa. The converted arrays don't share the same memory, because you don't want the Torch or numpy waits for each other to make operation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(5)\n",
    "y = np.arange(5)\n",
    "n_x = x.data.numpy()\n",
    "t_x = torch.from_numpy(y)\n",
    "print(type(n_x))\n",
    "print(type(t_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[2, 1, 4, 3],\n",
      "        [1, 2, 3, 4],\n",
      "        [4, 3, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).reshape((3,4))\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x < y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x > y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0],\n",
      "         [1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4],\n",
      "         [5]]])\n",
      "tensor([[0, 1]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6).reshape((2,3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [1, 2],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[3, 4],\n",
       "         [4, 5],\n",
       "         [5, 6]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5,5,2,1)\n",
    "y = torch.empty(    3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-cd60f97aa77f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting operations have to respect two rules:\n",
    "- The correspondent dimensions of vectors are equal, or\n",
    "- One of the correspondent dimensions are 1\n",
    "\n",
    "In the case above, 2 $\\neq$ 3, so the operation is impossible! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[2, 1, 4],\n",
      "        [1, 2, 3],\n",
      "        [4, 3, 2],\n",
      "        [3, 4, 1]])\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "id(out_mm): 140120977403120\n",
      "tensor([[ 19,  21,  11],\n",
      "        [ 59,  61,  51],\n",
      "        [ 99, 101,  91]])\n",
      "id(out_mm): 140120977403120\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(12).reshape((3,4))\n",
    "b = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "c = torch.ones(9).reshape((3,3)).long()\n",
    "print(a)\n",
    "print(b.t())\n",
    "print(c)\n",
    "out_mm = torch.zeros_like(c)\n",
    "print(\"id(out_mm):\",id(out_mm))\n",
    "torch.mm(a, b.t(), out = out_mm)\n",
    "torch.add(out_mm, c,  out = out_mm)\n",
    "print(out_mm)\n",
    "print(\"id(out_mm):\",id(out_mm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following sections will have some basics linear algebra. Algebra is one of the cores of machine learning concepts. Mathematical notations, codes, and explanations will fill the next sections of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalars are numbers in ${\\rm I\\!R}$ space that can be represented in mathematical notation using ordinary lower-case letters (x, y, z).  In equation like this: x = 10 + y, 10 is a scalar and x e y are variables representing unknown scalars.  Another way to said that scalars are in ${\\rm I\\!R}$ is a math notation x $\\in$ R, the symbol $\\in$ represents \"in\" and denotes membership in a set. <br>\n",
    "We can create scalars in Pytorch using torch.tensor. But, we have to pay attention to what type of scalar creates. The type of scalars makes a difference for training and evaluate models. Integers scalars are suitable for labels and classification, while float scalars are useful for regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.LongTensor\n",
      "x + y =  tensor([1.])\n",
      "x * y =  tensor([0.1600])\n",
      "x / y =  tensor([0.2500])\n",
      "x ** y =  tensor([0.2759])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'exponent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e0aad23b93bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x / y = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x ** y = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x ** y = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Differs type can be manipulate together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'exponent'"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0.2])\n",
    "b = torch.tensor([1])\n",
    "c = torch.tensor([0.8])\n",
    "print(a.type())\n",
    "print(b.type()) # Long int\n",
    "print('a + c = ', a + c)\n",
    "print('a * c = ', a * c)\n",
    "print('a / c = ', a / c)\n",
    "print('a ** c = ', torch.pow(a,c))\n",
    "print(a + b) # Differs type can be manipulate together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are a list of scalars. For example: [3.0, 0.4, 1.2]. Each scalar in a vector is known as entries or component.  Features in machine learning are represented using vectors. If the problem is predicting the value of a house, the features vector can be numbers of rooms, numbers of bathrooms, size, etc. Vectors are represented in math notation using lower-case bold letters, like this: (**x, v, b**).  In ***Pytorch*** vectors are tensors with multiple scalars, and these are in the same type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(5)\n",
    "print(x)\n",
    "print(x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors are zero-based numbering similar to lists and arrays.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Dimensionality, length and shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors have length, in another way, numbers of components.  In math notation vectors with n elements are in R^n-dim spaces.  So, here, we have some confuse concepts.  In python, the function len(), return the size of a vector, but, this size, can be accessed using the attribute shape. Tensors vectors in Pytorch only have one dimension, so, the attribute shape will return a tuple with one number. Matrix returns a tuple with two numbers and goes on. Scalars have 0-dimension. \n",
    "<br>\n",
    "*Note that the word dimension is overloaded and this tends to confuse people. Some use the dimensionality\n",
    "of a vector to refer to its length (the number of components). However, some use the word dimensionality to\n",
    "refer to the number of axes that an array has. In this sense, a scalar would have 0 dimensions, and a vector\n",
    "would have one dimension.\n",
    "To avoid confusion, when we say a 2D array or 3D array, we mean an array with 2 or 3 axes. Pag-45* **Dive into deep learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4, 5])\n",
      "tensor([12, 24, 36])\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "x = torch.tensor([1,2,3])\n",
    "y = torch.tensor([10,20,30])\n",
    "print(x + a)\n",
    "print(a * x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are 2D vectors and are represented by capital letters(*A, B, C*).  Matrices are a table, and each component are $a_{ij}$ belongs to i-th row and j-th column.  \n",
    "$$ \\begin{pmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\ \n",
    "a_{21} & a_{22} & a_{23} \\\\ \n",
    "\\end{pmatrix} $$\n",
    "<br> \n",
    "In Pytorch, matrices will create by specifying the two components of a tuple(n,m). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.ones((2,3))\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using reshape to modify a tensor vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.ones(6)\n",
    "print(m)\n",
    "m.reshape((2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are useful to store tabular with rows representing samples and columns representing features. In the example of house cited above, one row is a house with its features in columns. In Pytorch an entire row is accessed using one number(desire row) and colons. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.arange(6).reshape((2,3))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:,1] # Same for columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix can be transposed using t() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 4],\n",
       "        [2, 5]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5 Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Just as vectors generalize scalars, and matrices generalize vectors, we can actually build data structures\n",
    "with even more axes. Tensors give us a generic way of discussing arrays with an arbitrary number of axes.\n",
    "Vectors, for example, are first-order tensors, and matrices are second-order tensors. <br>\n",
    "Using tensors will become more important when we start working with images, which arrive as 3D data\n",
    "structures, with axes corresponding to the height, width, and the three (RGB) color channels. Pag-46* **Dive into deep learning**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([2, 3, 4])\n",
      "X = tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(24).reshape((2, 3, 4))\n",
    "print('X.shape =', X.shape)\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.6 Basic properties of tensor arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise operations don't change the shape of tensors. It same happens a scalar multiplies tensors. In math, these properties called AXPT operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "x = torch.ones(3)\n",
    "y = torch.ones(3)\n",
    "print(x.shape)\n",
    "print((a * x).shape)\n",
    "print((x + y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.7 Sums and means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some operations we can do with matrices and arrays. We can call *.sum()* to sum elements of an array or a matrix. We can calculate the mean of an array or a matrix using the function *.mean()*.  The symbol that represents sum is $\\sum$.  To express the sum of the vector components u with dimension d; we can write $\\sum_{i=1}^{d}a_{i}$. Mean is $\\frac{1}{d}\\sum_{i=1}^{d}a_{i}$. Sum of a marix $\\sum_{i=1}^{d_1}\\sum_{j=1}^{d_2}a_{ij}$. Mean of a matrix $\\frac{1}{d_1 \\times d_2}\\sum_{i=1}^{d_1}\\sum_{j=1}^{d_2}a_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n",
      "tensor(2.)\n",
      "tensor(66.)\n",
      "tensor(5.5000)\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1,2,3])\n",
    "b = torch.arange(12).reshape((3,4)).float() # Only calculate mean of float type\n",
    "print(a.sum()) \n",
    "print(a.mean())\n",
    "print(b.sum())\n",
    "print(b.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others possible operations:\n",
    "- std(Standard Deviation): *.std()*\n",
    "- var(Variance): *.var()*\n",
    "- unique(Unique values): *.unique()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5., 6.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1,2,3,1,4,1,1,1,5,6])\n",
    "a.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.8 Dot products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product is an essential operation in machine learning.  Given two vectors <b>u</b> and <b>v</b>, which has the same length,  we multiplication elements with the correspondent index and sum the multiplications. So the returned result is a scalar.  \n",
    "$$a \\cdot b = \\sum_{i}^{d}a_{i}b_{i} = a_{1}b_{1} + a_{2}b_{2} + \\cdots + a_{n}b_{n}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4.])\n",
      "tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(5).float()\n",
    "b = torch.tensor([2]).repeat_interleave(5).float() # Repeat elements of a vector N times\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.empty([])\n",
    "torch.dot(a,b,out=result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a * b) # Another way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are special cases when the dot product has context.  For example, when we have a vector with values that sum to 1 and are non-negative, and another vector, the dot product between these two vectors are a weighted average. \n",
    "$$ \\sum_{i}^{d}w_{i} = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.9 Matrix-vector products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix is a set of vectors. The matrix below can be represented in terms of its row vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = \\begin{pmatrix}\n",
    "a_{11} & a_{12} & a_{13}  \\\\ \n",
    "a_{21} & a_{22} & a_{23}  \\\\ \n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Representa as: \n",
    "$$ A = \\begin{pmatrix}\n",
    "a_{1}^{T} \\\\\n",
    "a_{2}^{T} \\\\\n",
    "a_{3}^{T} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$ x = \\begin{pmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3} \n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each $a_{i}^{T} \\in {\\rm I\\!R}^{m}$ is row representing the <i>i-th</i> row of matrix $A$. The result's vector of the multiplication between A and <b>x</b> is <b>y</b> $\\in {\\rm I\\!R}^{n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A\\textbf{x} = \\begin{pmatrix}\n",
    "a_{1}^{T} \\\\\n",
    "a_{2}^{T} \\\\\n",
    "a_{3}^{T} \n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3} \n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "a_{1}^{T}\\textbf{x} \\\\\n",
    "a_{2}^{T}\\textbf{x} \\\\\n",
    "a_{3}^{T}\\textbf{x} \n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see the the multiplication by a matrix $A  \\in {\\rm I\\!R}^{n\\times m}$ as a tranformation that projects vectors from ${\\rm I\\!R}^{m}$ to ${\\rm I\\!R}^{n}$. \n",
    "\n",
    "In pytorch we use the function *.mv()*, this means matrix-vector product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These transformations turn out to be remarkably useful. For example, we can represent rotations as multi-\n",
    "plications by a square matrix. As we will see in subsequent chapters, we can also use matrix-vector products\n",
    "to describe the calculations of each layer in a neural network. Pag-49* **Dive into deep learning**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]])\n",
      "tensor([2., 2., 2.])\n",
      "tensor([ 6., 24., 42.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(9).reshape((3,3)).float()\n",
    "b = torch.tensor([2]).repeat_interleave(3).float()\n",
    "result = torch.empty([])\n",
    "print(a)\n",
    "print(b)\n",
    "result = torch.mv(a,b,out=result) # .mv\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.10 Matrix-matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-matrix is similar to the section above. So if we have two matrices $A \\in {\\rm I\\!R}^{n\\times m}$ e $B \\in {\\rm I\\!R}^{m\\times k}$, the result is $C \\in {\\rm I\\!R}^{n\\times k}$ .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "tensor([[10., 13.],\n",
      "        [28., 40.],\n",
      "        [46., 67.],\n",
      "        [64., 94.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(12).reshape((4,3)).float()\n",
    "b = torch.arange(6).reshape((3,2)).float()\n",
    "result = torch.empty([])\n",
    "torch.mm(a,b,out=result)\n",
    "print(result.shape) # 4x3 and 3x2 transform to 4x2\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.11 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norms are essential concepts in machine learning. In machine learning, we are often trying to solve optimization problems:  Maximize the probability of observed data. Create representations of items likes words, sentences,  books, texts, products, in way that similar items have representations that minimize their distances.  These objectives are expressed using norms.  \n",
    "Norms can calculate how \"big\" is a vector or matrix. The representation of a norm is: $\\| \\cdot \\|$. So the norm of vector or matrix is: $\\| \\textbf{x}\\| - \\| A\\| $ respectivily.\n",
    "<br>\n",
    "All norms has to satisfy these properties:\n",
    "- $\\| \\alpha A\\|= |\\alpha|\\| A\\|$ (We multiply absolute value of scalar to norm)\n",
    "- $\\| A + B \\| \\leq \\| A \\| + \\| B \\|$ (Triangle inequality)\n",
    "- $\\| A \\| \\geq 0$ (All norms must be non-negative)\n",
    "- If $ \\forall i,j,a_{ij} = 0$, then $\\| A \\| = 0$ (The smallest norm is achieved by a matrix or vector consisting of all zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to have zero norm with non-zero matrices, but is impossible to have norm non-zero to zero matrices.\n",
    "\n",
    "\n",
    "*If you remember Euclidean distances (think Pythagoras’ theorem) from grade school, then non-negativity\n",
    "and the triangle inequality might ring a bell. You might notice that norms sound a lot like measures of\n",
    "distance. <br>\n",
    "In fact, the Euclidean distance $\\sqrt{x_{1}^{2} + \\cdots + x_{n}^{2}} $ is a norm. Specifically it is the L2-norm. An analogous\n",
    "computation, performed over the entries of a matrix, e.g. $\\sqrt{\\sum_{i,j}a_{i,j}^{2}}$ is called the Frobenius norm. More\n",
    "often, in machine learning we work with the squared L2 norm (notated $L_{2}^{2}$). We also commonly work with\n",
    "the L1 norm. The L1 norm is simply the sum of the absolute values. It has the convenient property of placing. Pag 50* **Dive into deep learning**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.12 Basic Vector Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Addtive axioms (x,y,z are vectors)\n",
    "    - x + y = y + x\n",
    "    - (x + y) + z = x + (y + z)\n",
    "    - 0 + x = x + 0 =  x\n",
    "    - (-x) + x = x + (-x) = 0\n",
    "- Multiplicative axioms (x is a vector and a, b are scalars)\n",
    "    - 0 $\\cdot$ x = 0\n",
    "    - 1 $\\cdot$ x = x\n",
    "    - (ab)x = a(bx)\n",
    "- Distributive axioms (x and y are vectors and a, b are scalars)\n",
    "    - a(x + y) = ax + ay\n",
    "    - (a + b)x = ax + bx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.13 Special matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Symmetric Matrix</b>: • Symmetric Matrix These are matrices where the entries below and above the diagonal are the same.\n",
    "In other words, we have that $M^{T} = M$. An example of such matrices are those that describe pairwise\n",
    "distances, i.e. $M_{ij} =\\|x_{i} −x_{j}\\|$. Likewise, the Facebook friendship graph can be written as a symmetric matrix where $M_{ij} = 1$ if i and j are friends and $M_{ij} = 0$ if they are not. Note that the Twitter graph\n",
    "is asymmetric - $M_{ij} = 1$, i.e. i following j does not imply that $M_{ji} = 1$, i.e. j following i.\n",
    "- <b>Antisymmetric Matrix</b>: These matrices satisfy $M^{T} = −M$. Note that any square matrix can always be decomposed into a symmetric and into an antisymmetric matrix by using $M = \\frac{1}{2}(M + M^{T}) + \\frac{1}{2}(M - M^{T}).$ \n",
    "- <b>Diagonally Dominant Matrix</b>: These are matrices where the off-diagonal elements are small relative\n",
    "to the main diagonal elements. In particular we have that $M_{ii} \\geq \\sum_{j \\neq i}M_{ij}$ and $M_{ii} \\geq \\sum_{j \\neq i} M_{ji}$. If a matrix has this property, we can often approximate M by its diagonal. This is often expressed as diag(M).\n",
    "• <b>Positive Definite Matrix</b> These are matrices that have the nice property where $x^{⊤}Mx > 0$ whenever $x \\neq 0$. Intuitively, they are a generalization of the squared norm of a vector $\\|x \\|^{2} = x^{⊤}x$. It is easy\n",
    "to check that whenever $M = A^{⊤}A$, this holds since there $x^{⊤}Mx = x^{⊤}A^{⊤}Ax = \\|Ax\\|^{2}$. There is a\n",
    "somewhat more profound theorem which states that all positive definite matrices can be written in form.\n",
    "\n",
    "These concepts above are essential to understand better machine learning. These properties and special matrices are widely used in fundamental machine learning concepts. Only go further if you understand all the concepts above.  Pag 52 **Dive into deep learning**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
