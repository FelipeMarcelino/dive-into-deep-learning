{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, the author introduces a few data manipulations.  These skills are necessary for any person entering in data science/machine learning.  Using PyTorch, and not MXnet, I had developed the same code/results showed in the book.  \n",
    "\n",
    "Linear algebra is the base of machine learning.  It gives us a robust set of techniques for working with tabular data.  Matrix operation is the core of machine learning,  principally using algorithms like backpropagation to optimization ours models parameters to fit our data as best possible, determining which way to optimize parameters of the models requires a little bit of calculus.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, data manipulation has two core tasks:\n",
    "- Acquire data\n",
    "- Process data\n",
    "Using the tensors of Pytorch, this kind of tool is necessary to store our data.  The tensors provide a few key advantages. First, it provides asynchronous computations using GPU and CPU.  Secondly, tensors can provide support for automatic differentiation. It is necessary for backpropagation and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter is focusing on getting you up and running with the basic functionality.  The next two chapters will be more concentrate on the math behind element-wise, normal distributions, and other essential operations.  In section 17.2, we have more in-depth mathematical content to be explored. <br><br>\n",
    "First, we need to import the Torch module.  In Torch we have tensors. Tensors are numerical arrays. It is like NDArrays of MXNet and can be stored in CPU or GPU. Tensors with two axes correspond to matrices, and arrays with more than two axes don't have any unique names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable x contains a tensor one-dimensional with length 12.  Another way to get the shape of a tensor is by using propriety .shape! If we have a two-dimensional array, the shape will be a tensor with two numbers! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function reshape allow us to change the shape of the tensors.  You can transform the one-dimensional tensor y into a matrix with shape (3,4).  If you don't want to make all calculations of dimensions, you can omit one dimension using the number -1 and writer the other dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "print(x.reshape((3,4)))\n",
    "print(x.reshape((-1,4)))\n",
    "print(x.reshape((3,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pytorch*** allows us to initialize tensors in multiple ways.  For example, you can initialize tensors with one's, zero's, or grab catch from memory.  The last method is more performative, but not too useful, because of the big numbers. Take a look of numbers [4.5710e-41, -2.4891e-37,  3.0899e-41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([0.0000e+00, 0.0000e+00, 7.0065e-45, 0.0000e+00, 8.9683e-44])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones(5))\n",
    "print(torch.zeros(5))\n",
    "print(torch.empty(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can randomly sample numbers from known distributions like gaussian or exponential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1.4635]), tensor([-0.7932]), tensor([-0.9193]), tensor([1.2948]), tensor([0.3475])]\n",
      "[tensor([0.0188]), tensor([0.8526]), tensor([0.6789]), tensor([1.2094]), tensor([0.0985])]\n"
     ]
    }
   ],
   "source": [
    "normal = torch.distributions.normal.Normal(torch.tensor([0.0]),torch.tensor([1.0]))\n",
    "exponential = torch.distributions.exponential.Exponential(torch.tensor([1.0]))\n",
    "sample_n = [normal.sample() for _ in range(5)]\n",
    "sample_e = [exponential.sample() for _ in range(5)]\n",
    "print(sample_n)\n",
    "print(sample_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet above, we created five samples of normal distributions with mean = 0 and std = 1($\\mathcal{N}(0,1)$) and five samples with exponential with rate = 1($\\lambda = 1$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([0, 1, 2, 3])\n",
      "y =  tensor([2, 2, 2, 2])\n",
      "x + y =  tensor([2, 3, 4, 5])\n",
      "x - y =  tensor([-2, -1,  0,  1])\n",
      "x * y =  tensor([0, 2, 4, 6])\n",
      "x / y =  tensor([0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "y = torch.ones_like(x) * 2\n",
    "print(\"x = \",x)\n",
    "print(\"y = \",y)\n",
    "print(\"x + y = \", x + y)\n",
    "print(\"x - y = \", x - y)\n",
    "print(\"x * y = \", x * y)\n",
    "print(\"x / y = \", x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations in Pytorch are element-wise. But, if you pay attention to div operation, it doesn't look right correct? It is because x and y are Long type tensors.  You have to transform these tensors in Float type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([0., 1., 2., 3.], dtype=torch.float64)\n",
      "y =  tensor([2., 2., 2., 2.], dtype=torch.float64)\n",
      "x + y =  tensor([2., 3., 4., 5.], dtype=torch.float64)\n",
      "x - y =  tensor([-2., -1.,  0.,  1.], dtype=torch.float64)\n",
      "x * y =  tensor([0., 2., 4., 6.], dtype=torch.float64)\n",
      "x / y =  tensor([0.0000, 0.5000, 1.0000, 1.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "y = torch.ones_like(x) * 2\n",
    "\n",
    "# Transformation\n",
    "x = x.double()\n",
    "y = y.double()\n",
    "\n",
    "print(\"x = \",x)\n",
    "print(\"y = \",y)\n",
    "print(\"x + y = \", x + y)\n",
    "print(\"x - y = \", x - y)\n",
    "print(\"x * y = \", x * y)\n",
    "print(\"x / y = \", x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many operations can be applied element-wise, such as exponentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0000,  2.7183,  7.3891, 20.0855], dtype=torch.float64)\n",
      "tensor([ 1.0000,  2.7183,  7.3891, 20.0855], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x = x.double()\n",
    "print(x.exp())\n",
    "print(torch.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***torch.mm*** allow us to made matrix operations. In the next code snippet, we create two matrices and transpose the second to make a dot multiplication between x and y. x has the shape (3,4), and y transpose has the shape(4,3), then creating a matrix with shape (3,3): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[2, 1, 4],\n",
      "        [1, 2, 3],\n",
      "        [4, 3, 2],\n",
      "        [3, 4, 1]])\n",
      "tensor([[ 18,  20,  10],\n",
      "        [ 58,  60,  50],\n",
      "        [ 98, 100,  90]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).reshape((3,4))\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(x)\n",
    "print(y.t())\n",
    "print(torch.mm(x,y.t()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another operations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [ 2,  1,  4,  3],\n",
      "        [ 1,  2,  3,  4],\n",
      "        [ 4,  3,  2,  1]])\n",
      "tensor([[ 0,  1,  2,  3,  2,  1,  4,  3],\n",
      "        [ 4,  5,  6,  7,  1,  2,  3,  4],\n",
      "        [ 8,  9, 10, 11,  4,  3,  2,  1]])\n",
      "tensor([[0, 1, 0, 1],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], dtype=torch.uint8)\n",
      "tensor(66)\n",
      "22.494443758403985\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat((x,y),0)) # Concatenation along axes 0\n",
    "print(torch.cat((x,y),1)) # Concatenation along axes 0\n",
    "print(x == y) # Binary statement: if x(i,j) == y(i,j) than 1, else 0\n",
    "print(x.sum())\n",
    "print(x.double().norm().item()) # Only for floating-point types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function .item() transform the tensor into python scalar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Broadcast Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]]) torch.Size([3, 1])\n",
      "tensor([[0, 1]]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "print(a,a.shape)\n",
    "print(b,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The broadcast mechanism is similar to Numpy. First, replicate the elements in rows and columns, so the two tensors have the same shape, and then apply the operations by elements.  Above Pytorch replicate column of tensor a and row b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 Indexing and slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Python array, elements in tensor can be accessed by its index.  One example, x[0:3] select the first element to last - 1 element, in that case, items [0,1,2] will be chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1,4)[1:3] # Matrix: Selects second and third row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1,4)[1:3,0:2] # Matrix: Selects second and third row and first and second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5, -1,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [-2, -2, -2, -2],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "x_diff = x.reshape(-1,4)\n",
    "x_diff[1,2] = -1\n",
    "print(x_diff)\n",
    "x_diff[1,:] = -2\n",
    "print(x_diff) #Assign multiple times in the second row and all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write elements of a matrix. Like above! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 Saving memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving memory is useful when we have restricted memory.  The last operations made in this notebooks, we always allocate new memory to host results.  In the example below,  y = x + y, the matrix pointed to y will be different after you get the result. Python id() function gives us the exact address of the referenced object in memory.  First, evaluates y + x, allocate new memory for the result and then subsequently redirects y to point at this new location in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(y)\n",
    "y = y.reshape(12) + x\n",
    "id(y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using inplace operations, we can have the same space of memory to store our results and avoid memory leak and unnecessarily allocation of memory.  Using zeros_likes, we clone the shape of a matrix and allocate zeros values into this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(z): 140120981356120\n",
      "id(z): 140120981356120\n"
     ]
    }
   ],
   "source": [
    "z = torch.zeros_like(y)\n",
    "print('id(z):', id(z))\n",
    "z[:] = x + y\n",
    "print('id(z):', id(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make even better use of memory, because of x + y here still allocate a temporary buffer to store x + y, we can directly invoke torch operations, avoiding temporary buffers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(z)\n",
    "torch.add(x, y, out=z)\n",
    "id(z) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to make in-place operations are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(x): 140120981561848\n",
      "tensor([0, 2, 4, 6]) id(x): 140120981561848\n",
      "id(x): 140121834222216\n",
      "tensor([0, 2, 4, 6]) id(x): 140121834222216\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "y = torch.arange(4)\n",
    "print(\"id(x):\", id(x))\n",
    "x += y\n",
    "print(x,\"id(x):\",id(x))\n",
    "x = torch.arange(4)\n",
    "y = torch.arange(4)\n",
    "print(\"id(x):\",id(x))\n",
    "x.add_(y)\n",
    "print(x,\"id(x):\",id(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!! Careful !!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-place operations for autograd is ***dangerous!*** <br>\n",
    "https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.6 Mututal Transformation of NDArray and NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last subsection is only a minor and easy example of converting a numpy array to tensor and vice-versa. The converted arrays don't share the same memory, because you don't want the Torch or numpy waits for each other to make operation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(5)\n",
    "y = np.arange(5)\n",
    "n_x = x.data.numpy()\n",
    "t_x = torch.from_numpy(y)\n",
    "print(type(n_x))\n",
    "print(type(t_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[2, 1, 4, 3],\n",
      "        [1, 2, 3, 4],\n",
      "        [4, 3, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).reshape((3,4))\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x < y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x > y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0],\n",
      "         [1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4],\n",
      "         [5]]])\n",
      "tensor([[0, 1]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6).reshape((2,3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [1, 2],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[3, 4],\n",
       "         [4, 5],\n",
       "         [5, 6]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5,5,2,1)\n",
    "y = torch.empty(    3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-cd60f97aa77f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting operations have to respect two rules:\n",
    "- The correspondent dimensions of vectors are equal, or\n",
    "- One of the correspondent dimensions are 1\n",
    "\n",
    "In the case above, 2 $\\neq$ 3, so the operation is impossible! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[2, 1, 4],\n",
      "        [1, 2, 3],\n",
      "        [4, 3, 2],\n",
      "        [3, 4, 1]])\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "id(out_mm): 140120977403120\n",
      "tensor([[ 19,  21,  11],\n",
      "        [ 59,  61,  51],\n",
      "        [ 99, 101,  91]])\n",
      "id(out_mm): 140120977403120\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(12).reshape((3,4))\n",
    "b = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "c = torch.ones(9).reshape((3,3)).long()\n",
    "print(a)\n",
    "print(b.t())\n",
    "print(c)\n",
    "out_mm = torch.zeros_like(c)\n",
    "print(\"id(out_mm):\",id(out_mm))\n",
    "torch.mm(a, b.t(), out = out_mm)\n",
    "torch.add(out_mm, c,  out = out_mm)\n",
    "print(out_mm)\n",
    "print(\"id(out_mm):\",id(out_mm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following sections will have some basics linear algebra. Algebra is one of the cores of machine learning concepts. Mathematical notations, codes, and explanations will fill the next sections of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalars are numbers in ${\\rm I\\!R}$ space that can be represented in mathematical notation using ordinary lower-case letters (x, y, z).  In equation like this: x = 10 + y, 10 is a scalar and x e y are variables representing unknown scalars.  Another way to said that scalars are in ${\\rm I\\!R}$ is a math notation x $\\in$ R, the symbol $\\in$ represents \"in\" and denotes membership in a set. <br>\n",
    "We can create scalars in Pytorch using torch.tensor. But, we have to pay attention to what type of scalar creates. The type of scalars makes a difference for training and evaluate models. Integers scalars are suitable for labels and classification, while float scalars are useful for regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.LongTensor\n",
      "x + y =  tensor([1.])\n",
      "x * y =  tensor([0.1600])\n",
      "x / y =  tensor([0.2500])\n",
      "x ** y =  tensor([0.2759])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'exponent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e0aad23b93bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x / y = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x ** y = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x ** y = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Differs type can be manipulate together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'exponent'"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0.2])\n",
    "b = torch.tensor([1])\n",
    "c = torch.tensor([0.8])\n",
    "print(a.type())\n",
    "print(b.type()) # Long int\n",
    "print('a + c = ', a + c)\n",
    "print('a * c = ', a * c)\n",
    "print('a / c = ', a / c)\n",
    "print('a ** c = ', torch.pow(a,c))\n",
    "print(a + b) # Differs type can be manipulate together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are a list of scalars. For example: [3.0, 0.4, 1.2]. Each scalar in a vector is known as entries or component.  Features in machine learning are represented using vectors. If the problem is predicting the value of a house, the features vector can be numbers of rooms, numbers of bathrooms, size, etc. Vectors are represented in math notation using lower-case bold letters, like this: (**x, v, b**).  In ***Pytorch*** vectors are tensors with multiple scalars, and these are in the same type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(5)\n",
    "print(x)\n",
    "print(x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors are zero-based numbering similar to lists and arrays.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Dimensionality, length and shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors have length, in another way, numbers of components.  In math notation vectors with n elements are in R^n-dim spaces.  So, here, we have some confuse concepts.  In python, the function len(), return the size of a vector, but, this size, can be accessed using the attribute shape. Tensors vectors in Pytorch only have one dimension, so, the attribute shape will return a tuple with one number. Matrix returns a tuple with two numbers and goes on. Scalars have 0-dimension. \n",
    "<br>\n",
    "*Note that the word dimension is overloaded and this tends to confuse people. Some use the dimensionality\n",
    "of a vector to refer to its length (the number of components). However, some use the word dimensionality to\n",
    "refer to the number of axes that an array has. In this sense, a scalar would have 0 dimensions, and a vector\n",
    "would have one dimension.\n",
    "To avoid confusion, when we say a 2D array or 3D array, we mean an array with 2 or 3 axes. Pag-45* **Dive into deep learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4, 5])\n",
      "tensor([12, 24, 36])\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "x = torch.tensor([1,2,3])\n",
    "y = torch.tensor([10,20,30])\n",
    "print(x + a)\n",
    "print(a * x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are 2D vectors and are represented by capital letters(*A, B, C*).  Matrices are a table, and each component are $a_{ij}$ belongs to i-th row and j-th column.  \n",
    "$$ \\begin{pmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\ \n",
    "a_{21} & a_{22} & a_{23} \\\\ \n",
    "\\end{pmatrix} $$\n",
    "<br> \n",
    "In Pytorch, matrices will create by specifying the two components of a tuple(n,m). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.ones((2,3))\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using reshape to modify a tensor vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.ones(6)\n",
    "print(m)\n",
    "m.reshape((2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are useful to store tabular with rows representing samples and columns representing features. In the example of house cited above, one row is a house with its features in columns. In Pytorch an entire row is accessed using one number(desire row) and colons. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.arange(6).reshape((2,3))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:,1] # Same for columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix can be transposed using t() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 4],\n",
       "        [2, 5]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5 Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Just as vectors generalize scalars, and matrices generalize vectors, we can actually build data structures\n",
    "with even more axes. Tensors give us a generic way of discussing arrays with an arbitrary number of axes.\n",
    "Vectors, for example, are first-order tensors, and matrices are second-order tensors. <br>\n",
    "Using tensors will become more important when we start working with images, which arrive as 3D data\n",
    "structures, with axes corresponding to the height, width, and the three (RGB) color channels. Pag-46* **Dive into deep learning**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([2, 3, 4])\n",
      "X = tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(24).reshape((2, 3, 4))\n",
    "print('X.shape =', X.shape)\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic properties of tensor arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise operations don't change the shape of tensors. It same happens a scalar multiplies tensors. In math, these properties called AXPT operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "x = torch.ones(3)\n",
    "a * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
